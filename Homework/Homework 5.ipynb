{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "southern-district",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "\n",
    "(100 points) Consider the following problem. \n",
    "\n",
    "$$\n",
    "\\hbox{min } f =  x_1^2+(x_2-3)^2\n",
    "$$\n",
    "$$\n",
    "\\hbox{s.t. } g_1 =  x_2^2-2x_1\\leq 0\n",
    "$$\n",
    "$$\n",
    " g_2= (x_2-1)^2+5x_1-15\\leq 0\n",
    "$$\n",
    "\n",
    "Implement an SQP algorithm with line search to solve this problem, starting from\n",
    "${\\bf x}_0=(1,1)^T$. Incorporate the QP subproblem. Use BFGS\n",
    "approximation for the Hessian of the Lagrangian. Use the\n",
    "merit function and Armijo Line Search to find the step size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36079ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import scipy.optimize as opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6d115f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# function defs\n",
    "\n",
    "def cost_f(x):\n",
    "    return x[0]**2.0 + (x[1] - 3)**2.0\n",
    "\n",
    "def g_1(x):\n",
    "    return x[1]**2.0 - 2.0 * x[0]\n",
    "\n",
    "def g_2(x):\n",
    "    return (x[1] - 1)**2.0 + 5.0 * x[0] - 15.0\n",
    "\n",
    "def g_f(x):\n",
    "    return [g_1(x), g_2(x)]\n",
    "\n",
    "def grad_cost_f(x):\n",
    "    return np.array([2.0 * x[0], 2.0 * x[1] - 6.0])\n",
    "\n",
    "def grad_g_f(x):\n",
    "    return np.array([[-2.0, 5.0], [2.0 * x[1], 2.0 * x[1] - 2.0]])\n",
    "\n",
    "'''\n",
    "def line_search_phi(x, params):\n",
    "    #phi = cost(p) - params['a'] * params['t'][0] * np.dot(grad(p), grad(p))\n",
    "    phi = cost_f(x) - params['a'] * np.dot(params['t'], grad_cost_f(x))\n",
    "    return phi\n",
    "    \n",
    "def line_search(x, params):\n",
    "    a = params['a']\n",
    "    i = 0\n",
    "    while line_search_phi(x, params) < cost_f(x - a * grad_cost_f(x)):\n",
    "        a = 0.5 * a\n",
    "        i = i + 1\n",
    "\n",
    "        if params['debug_verbose']:\n",
    "            print(\"line_search_phi: \" + str(line_search_phi(x, params)) + \", cost: \" + str(cost_f(x - a * grad_cost_f(x))) + \", a: \" + str(a))\n",
    "            print(\"p: \" + str(x) + \"grad: \" + str(grad_cost_f(x)))\n",
    "\n",
    "        if(params['max_i'] < i):\n",
    "            if params['verbose']:\n",
    "                print(\"!!Iteration limit reached in line_search, returning a of \" + str(a) + \"!!\")\n",
    "                print(\"line_search_phi: \" + str(line_search_phi(x, params)) + \", cost: \" + str(cost_f(x - a * grad_cost_f(x))) + \", a: \" + str(a))\n",
    "            return a\n",
    "    return a\n",
    "'''\n",
    "def QP(x, lam, h, W):\n",
    "    max_iter_count = 100\n",
    "    for i in range(max_iter_count):\n",
    "        A = grad_g_f(x)\n",
    "        fx = grad_cost_f(x)\n",
    "        h = g_f(x)\n",
    "        big_mat = np.array([[W, np.transpose(A)], [A, np.zeros(size=np.size(A))]])\n",
    "        s_lam = np.linalg.pinv(big_mat) @ np.array([-fx, -h])\n",
    "        s = s_lam[0:2]\n",
    "        lam = s_lam[2:4]\n",
    "        mu_mag = np.linalg.norm(lam) # mu and lam are interchangeable here, no equality constraints\n",
    "        if mu_mag > 0:\n",
    "            return s, lam\n",
    "        elif mu_mag <= 0: # add more here\n",
    "            pass\n",
    "    print(\"QP hit max iteration count!\")\n",
    "    return [s, lam]\n",
    "\n",
    "def line_search(s, lam, mu, f, h, g, A):\n",
    "    return 0.2 # not yet implimented\n",
    "\n",
    "def BFGS(w_k, alpha_s, x, lam, mu, f, h, g, A):\n",
    "    hessian_appx = np.eye(size=np.size(x))\n",
    "    return hessian_appx\n",
    "\n",
    "def SQP(x_0, lam_0, mu_0, W_0, epi, A):\n",
    "    div_L = cost_f(x_k) + np.transpose(mu_k) @ grad_g_f(x_k)\n",
    "    norm_div_L = np.linalg.norm(div_L)\n",
    "    while epi < norm_div_L:\n",
    "        (s_k, lam_mu) = QP()\n",
    "        alpha_k = line_search()\n",
    "        x_kp1 = x_k + alpha_k * s_k\n",
    "        W_est_kp1 = BFGS()\n",
    "        div_L = cost_f(x_kp1) + np.transpose(x_kp1) @ grad_g_f(x_kp1)\n",
    "        norm_div_L = np.linalg.norm(div_L)\n",
    "    return x_kp1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac962f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# checking real answer \n",
    "x_0 = np.array([1.0, 1.0])\n",
    "constraints = opt.NonlinearConstraint(g_f, [-np.inf, -np.inf], [0.0, 0.0])\n",
    "result = opt.minimize(cost_f, x_0, constraints=constraints)\n",
    "print(result.message)\n",
    "print(result.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a0a37de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully\n",
      "[1.06020715 1.45616424]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "615c8154",
   "metadata": {},
   "source": [
    "* DONE\n",
    "    * Set up basic structure\n",
    "    * most of QP\n",
    "    * checked what answer I should be getting.\n",
    "\n",
    "* TODO \n",
    "    * check QP\n",
    "    * impliment merit function line search\n",
    "    * impliment BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-niger",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
