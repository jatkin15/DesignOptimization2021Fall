{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Theory/Computation Problems\n",
    "\n",
    "### Problem 1 (20 points) \n",
    "Show that the stationary point (zero gradient) of the function\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "is a saddle (with indefinite Hessian). Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$.\n",
    "\n",
    "### Problem 2 (50 points) \n",
    "\n",
    "* (10 points) Find the point in the plane $x_1+2x_2+3x_3=1$ in $\\mathbb{R}^3$ that is nearest to the point $(-1,0,1)^T$. Is this a convex problem? Hint: Convert the problem into an unconstrained problem using $x_1+2x_2+3x_3=1$.\n",
    "\n",
    "* (40 points) Implement the gradient descent and Newton's algorithm for solving the problem. Attach your codes along with a short summary including (1) the initial points tested, (2) corresponding solutions, (3) a log-linear convergence plot.\n",
    "\n",
    "### Problem 3 (10 points) \n",
    "Let $f(x)$ and $g(x)$ be two convex functions defined on the convex set $\\mathcal{X}$. \n",
    "* (5 points) Prove that $af(x)+bg(x)$ is convex for $a>0$ and $b>0$. \n",
    "* (5 points) In what conditions will $f(g(x))$ be convex?\n",
    "\n",
    "### Problem 4 (bonus 10 points)\n",
    "Show that $f({\\bf x}_1) \\geq f(\\textbf{x}_0) + \n",
    "    \\textbf{g}_{\\textbf{x}_0}^T(\\textbf{x}_1-\\textbf{x}_0)$ for a convex function $f(\\textbf{x}): \\mathcal{X} \\rightarrow \\mathbb{R}$ and for $\\textbf{x}_0$, $\\textbf{x}_1 \\in \\mathcal{X}$. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Design Problems\n",
    "\n",
    "### Problem 5 (20 points) \n",
    "Consider an illumination problem: There are $n$ lamps and $m$ mirrors fixed to the ground. The target reflection intensity level is $I_t$. The actual reflection intensity level on the $k$th mirror can be computed as $\\textbf{a}_k^T \\textbf{p}$, where $\\textbf{a}_k$ is given by the distances between all lamps to the mirror, and $\\textbf{p}:=[p_1,...,p_n]^T$ are the power output of the lamps. The objective is to keep the actual intensity levels as close to the target as possible by tuning the power output $\\textbf{p}$.\n",
    "\n",
    "* (5 points) Formulate this problem as an optimization problem. \n",
    "* (5 points) Is your problem convex?\n",
    "* (5 points) If we require the overall power output of any of the $n$ lamps to be less than $p^*$, will the problem have a unique solution?\n",
    "* (5 points) If we require no more than half of the lamps to be switched on, will the problem have a unique solution?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 1\n",
    "The given function of \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    f=2x_{1}^{2} - 4x_1 x_2+ 1.5x^{2}_{2}+ x_2\n",
    "\\end{aligned}\n",
    "$$\n",
    "has a gradient g of \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    g=\\begin{bmatrix}\n",
    "    4x_{1} - 4x_2 \\\\\n",
    "    -4x_1 + 3 x_2 + 1 \n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "Setting this equal to the zero vector to find the stationary point, we see\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    g=\\begin{bmatrix}\n",
    "    4x_{1} - 4x_2 \\\\\n",
    "    -4x_1 + 3 x_2 + 1 \n",
    "    \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\to \n",
    "    x_1 = x_2 \\to -4x_1 + 3x_1 + 1 = 0 \\to x_1 = 1\n",
    "\\end{aligned}\n",
    "$$\n",
    "which means the stationary point is at $ x_0 = (1, 1)$.\n",
    "\n",
    "Computing the Hessian, H, we find\n",
    "$$\n",
    "    \\begin{aligned}\n",
    "    H=\\begin{bmatrix}\n",
    "    4 & -4 \\\\\n",
    "    -4 & 3\n",
    "    \\end{bmatrix}\n",
    "\\end{aligned}\n",
    "$$\n",
    "which we can then find the two eigenvalues of $\\lambda_1 = -0.53113$ and $\\lambda_2 = 7.53113$.\n",
    "\n",
    "Since the eigenvalues have a mix of positive and negative signs, this implies that the Hessian is indefinite and the stationary point of $x_0 = (1, 1)$ is also a saddle point.\n",
    "\n",
    "## TODO: Find the directions of downslopes away from the saddle. Hint: Use Taylor's expansion at the saddle point. Find directions that reduce $f$.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 2\n",
    "\n",
    "## TODO part 1, part 2"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 3\n",
    "\n",
    "## Part 1\n",
    "For the first question, we can use the definition of a convex function to check if their (positive) weighted sum is also convex to get the inequality \n",
    "$$\n",
    "a f(\\lambda x_1 + (1-\\lambda) x_2) + b g(\\lambda x_1 + (1-\\lambda) x_2) \\leq a \\lambda f(x_1) + a (\\lambda - 1) f(x_2) + b \\lambda g(x_1) + b (\\lambda - 1) g(x_2)\n",
    "$$\n",
    "This can be rearranged into\n",
    "$$\n",
    "a f(\\lambda x_1 + (1-\\lambda) x_2) + b g(\\lambda x_1 + (1-\\lambda) x_2) \\leq a (\\lambda f(x_1) +  (\\lambda - 1) f(x_2)) + b (\\lambda g(x_1) + (\\lambda - 1) g(x_2))\n",
    "$$\n",
    "We see that the condition of $f(\\lambda x_1 + (1-\\lambda) x_2) \\leq \\lambda f(x_1) +  (\\lambda - 1) f(x_2)$ and similar for $g(x)$ is simply weigthed by $a$ and $b$ respectively. Since $f(x)$ and $g(x)$ are known to be convex, these conditions hold seperately as the weights can be divided out, or\n",
    "$$\n",
    "a f(\\lambda x_1 + (1-\\lambda) x_2) \\leq a \\lambda f(x_1) +  (\\lambda - 1) f(x_2) \\implies f(\\lambda x_1 + (1-\\lambda) x_2) \\leq \\lambda f(x_1) +  (\\lambda - 1) f(x_2)\n",
    "$$\n",
    "meaning the strictly positive weights $a$ and $b$ do not alter the convexity of $f(x)$ and $g(x)$ individually. \n",
    "\n",
    "Furthermore, since $a f(x)$ and $b g(x)$ are convex, we can add their inequalities to see that their sum is also convex as the inequality is not altered.\n",
    "$$\n",
    "[a f(\\lambda x_1 + (1-\\lambda) x_2)] + [b g(\\lambda x_1 + (1-\\lambda) x_2)] \\leq [a (\\lambda f(x_1) +  (\\lambda - 1) f(x_2))] + [b (\\lambda g(x_1) + (\\lambda - 1) g(x_2))]\n",
    "$$\n",
    "\n",
    "Therefore, the weights $a$ and $b$ do not alter the convexity of $f(x)$ or $g(x)$ individually which implies that the sum $a f(x) + b g(x)$ satisifies the inequality condition for convexity as it is not altered by adding the two individual inequalities which then implies $af(x) + b g(x)$ is convex. \n",
    "\n",
    "## Part 2\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 4\n",
    "## TODO"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Problem 5\n",
    "## Part 1\n",
    "Since the objective is to keep the minimize the difference between the $I$ and $a_k^T p$ by adjusting $p$, we can write this as an unconstrained optimization problem as\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\min_{p} \\quad \\Sigma_{k=1}^m (a_k^T p - I)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "## TODO Give more detail\n",
    "\n",
    "## Part 2\n",
    "The problem is convex if the Hessian is positive semi-definite or strictly convex if positive definite, so checking the eigenvalues of the Hessian of the cost function can show this. \n",
    "\n",
    "Starting with a single $k$ mirror of the cost function, we see the gradient as\n",
    "$$\n",
    "    g_k = \\nabla (a_k^T p - I)^2 = 2 (a_k p - I) a_k^T\n",
    "$$\n",
    "Then the Hessian is the gradient of the gradient, or\n",
    "$$\n",
    "    H_k = \\nabla g = \\nabla 2 (a_k p - I) a_k^T = 2 a_k a_k^T\n",
    "$$\n",
    "Since each term in the summation of the cost function is entirely seperable, we have\n",
    "$$\n",
    "    H = \\Sigma_{k=1}^m H_k = \\Sigma_{k=1}^m 2 a_k a_k^T\n",
    "$$\n",
    "Note that the physical meaning of $a_k$ being a $1 \\times n$ vector of distances and distances are non-negative, all the elements of each $a_k$ would also be non-negative so the problem would at least be convex.\n",
    "\n",
    "## TODO Give more detail, Part 3, Part 4\n",
    "\n",
    "## Part 3\n",
    "\n",
    "## Part 4\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}